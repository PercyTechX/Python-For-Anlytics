# -*- coding: utf-8 -*-
"""22_ Sesión_14_Modelos_Avanzados_de_Clasificación.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11NJUbajTg2FqUjkF83-etAIrEZQPICmI

# **Modelos Avanzados de Clasificación**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

url = "https://raw.githubusercontent.com/JBrianAlicorp/Business-Analytics/master/Data_Customer_Churn.csv"

url

data = pd.read_csv(url,encoding = "latin1")

data.head()

data.info()

data.duplicated().sum()

"""# **Análisis Descriptivo**"""

data["Churn"].value_counts()

plt.pie(data["Churn"].value_counts(), autopct= "%1.1f%%",labels = data["Churn"].unique())

"""# **Preprocesamiento de data**"""

data.head()

data.info()

data["TotalCharges"] = data["TotalCharges"].replace(" ", np.nan)

data.dropna(inplace = True)

data["TotalCharges"] = data["TotalCharges"].astype("float")

cat_cols = [x for x in data.columns if data[x].nunique()<6 and x!= "Churn"]

cat_cols

num_cols = [x for x in data.columns if data[x].nunique()>=6 and x!= "customerID"]

num_cols

id_customer = data["customerID"]
label = data["Churn"]

label.head()

label = label.apply(lambda x: 1 if x =="Yes" else 0)

label.head()

from sklearn.preprocessing import MinMaxScaler

features_log_transformed = pd.DataFrame(data = data[num_cols])

features_log_transformed[num_cols] = data[num_cols].apply(lambda x: np.log(x+1))

scaler = MinMaxScaler()

features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)

features_log_minmax_transform[num_cols] = scaler.fit_transform(features_log_transformed[num_cols])

features_log_minmax_transform.describe()

sns.heatmap(features_log_minmax_transform.corr(), annot = True)

features_log_minmax_transform.drop("tenure", inplace=True, axis=1)

data.drop(["MonthlyCharges","TotalCharges","tenure"], axis = 1, inplace = True)

data = pd.concat([data, features_log_minmax_transform], axis=1)

data.head()

#data.drop("Churn", inplace=True, axis=1)
data.drop("customerID", inplace=True, axis=1)

data.info()

data = pd.get_dummies(data = data, columns= cat_cols)

data.head()

label

data_original = pd.concat([data, label], axis=1)

data_original.head()

"""# **Evaluación de Algoritmos**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test= train_test_split(data, label, test_size = 0.3)

X_train.shape

X_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score, roc_curve

def apply_classifier(clf,xTrain,xTest,yTrain,yTest):

    clf.fit(xTrain,yTrain) #Entrenamiento del modelo
    predictions = clf.predict(xTest) #Validación sobre la data del testeo
    conf_mtx = confusion_matrix(yTest, predictions) #Matriz de confusión de la data de testeo
    f, axes = plt.subplots(ncols = 2, figsize = (15,6)) #Definición del área de visualización de resultados (2 gráficas)
    sns.heatmap(conf_mtx, annot=True, cbar= False, fmt="g", ax = axes[0]) # Definición del formato de la matriz de confusión
    axes[0].set_xlabel("Predicciones") #Título del eje x de la matriz de confusión
    axes[0].set_ylabel("Datos Reales") #Título del eje y de la matriz de confusión
    axes[0].set_title("Matriz de confusión") # Título de la matriz de confusión
    axes[0].xaxis.set_ticklabels(["Not Churn", "Churn"]) #Distribución de los resultados en la matriz de confusión
    axes[0].yaxis.set_ticklabels(["Not Churn", "Churn"]) #Distribución de los resultados en la matriz de confusión

    print("Reporte de Clasificación : {}".format(classification_report(yTest, predictions)))

    roc_auc = roc_auc_score(yTest, predictions) #Definición del cálculo del auc
    print("Area bajo la curva ROC:", roc_auc) # Mostrar el resultado

    fpr, tpr,_ = roc_curve(yTest, predictions) #Función para armar la curva ROC
    axes[1].plot(fpr, tpr, label = "auc"+str(roc_auc)) #código para mostar el resultado de la curva AUC
    axes[1].plot([0,1],[0,1], lw=1) #código para definir el formato de la gráfica
    plt.xlim([0,1]) #dimensión del eje x
    plt.ylim([0,1]) #dimensión del eje y
    plt.xlabel(["Ratio Falsos Positivos"]) #título del eje x
    plt.ylabel(["Ratio Verdaderos Positivos"]) #título del eje y
    plt.title("CURVA ROC") #título del gráfico
    plt.legend(loc="Umbral") #para mostrar leyenda

decision_tree = DecisionTreeClassifier()

apply_classifier(decision_tree,X_train,X_test,y_train,y_test)

logistic_reg = LogisticRegression()

apply_classifier(logistic_reg, X_train,X_test, y_train, y_test)

svm_model = SVC()
apply_classifier(svm_model,X_train, X_test, y_train, y_test)

random_forest = RandomForestClassifier()
apply_classifier(random_forest, X_train, X_test, y_train, y_test)

xg_boost = XGBClassifier()
apply_classifier(xg_boost, X_train, X_test, y_train, y_test)

"""# **Calibración de hiperparámetros**"""

Tree_parameters = {"max_depth":[3,4,5,6], "min_samples_leaf": [1,2,3,4]}

LogReg_parameters = {"C": [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 10.0],
                     "solver": ["newton-cg", "lbfgs", "sag", "saga"],
                     "tol": [0.01, 0.001, 0.0001, 0.00001]}

SVM_parameters = {"C": [1.0, 2.0, 3.0], "kernel":["sigmoid", "linear"], "tol": [0.01, 0.001, 0.0001, 0.00001]}

RandomForest_parameters = {"n_estimators": [10,15, 20, 25, 30], "criterion": ["entropy", "gini"],"max_depth":[3,4,5,6]}

Xgboost_parameters = {"max_depth":[3,4,5,6], "learning_rate":[0.001, 0.0001], "min_child_weight": [1,2,3,4]}

from sklearn.model_selection import GridSearchCV

def grid_search(clf, parameters, xTrain, Ytrain):

    grid_obj = GridSearchCV(clf, parameters, scoring="roc_auc", cv=5)
    grid_fit = grid_obj.fit(xTrain, Ytrain)
    best_clf = grid_fit.best_estimator_

    return best_clf

tree_grid = grid_search(decision_tree, Tree_parameters, X_train, y_train)
apply_classifier(tree_grid, X_train, X_test, y_train, y_test)

logReg_grid = grid_search(logistic_reg, LogReg_parameters, X_train, y_train)
apply_classifier(logReg_grid, X_train, X_test, y_train, y_test)

